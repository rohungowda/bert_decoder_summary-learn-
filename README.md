# bert_decoder_summary-learn
This file contains my research into using BERT as an encoder for large portions of text and then feeding them through a transformer decoder layer to see if i can generate relevant summaris. However, no chance to test it as needs much more resource computation than available. However, it was a great learning oppurtunity as I learned more about the following:

BERT
BERT tokenizer
Transformer deocder Layers
Truncation Methods

Feature: Built transformer decoder layer and functionality that allowed previous output to be right shifted and be used as new tokens inot the decoder as well as taking BERT hidden embeddings as encoder information.
(learn) - part of a learning series I am doing to self teach myself deep learning and generative AI 
